# üöÄ AI Cost Optimization Features

## üìã **Overview**

‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI Generation ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å‡∏î‡πâ‡∏≤‡∏ô:

- ‚úÖ **Caching System** - ‡∏•‡∏î API calls ‡∏ã‡πâ‡∏≥
- ‚úÖ **Rate Limiting** - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô
- ‚úÖ **Token Monitoring** - ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ token
- ‚úÖ **Cost Estimation** - ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢
- ‚úÖ **Prompt Optimization** - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á prompt ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û

## üèóÔ∏è **Architecture**

```
User Request ‚Üí Rate Limit Check ‚Üí Cache Check ‚Üí OpenAI API ‚Üí Token Tracking
     ‚Üì              ‚Üì              ‚Üì           ‚Üì           ‚Üì
Rate Limiting   Caching      Token Usage   Cost Calc   Monitoring
```

## üîß **Components Implemented**

### 1. **CacheConfig.java**
- Caffeine cache configuration
- Cache size: 1000 entries
- Expire after: 1 hour (write), 30 minutes (access)
- Cache names: ai-flashcard, ai-quiz, ai-note, ai-summary

### 2. **RateLimitService.java**
- User-based rate limiting
- Limits: minute, hourly, daily
- Configurable via application.yml
- Automatic reset at intervals

### 3. **TokenUsageService.java**
- Track input/output tokens
- Cost estimation (GPT-5 Nano pricing)
- Daily/monthly projections
- User-specific tracking

### 4. **CachedAIService.java**
- Intelligent caching with SHA-256 keys
- Cache similar requests
- Reduce duplicate API calls

### 5. **AIMonitoringController.java**
- REST endpoints for monitoring
- Rate limit status
- Token usage statistics
- Cost optimization tips

## üìä **Configuration**

### **application.yml**
```yaml
ai:
  generation:
    rate-limit:
      max-requests-per-hour: 5
      max-requests-per-minute: 3
      max-requests-per-day: 50
```

### **Environment Variables**
```bash
export OPENAI_MODEL="gpt-5-nano"
export OPENAI_MAX_TOKENS="256"
export OPENAI_TEMPERATURE="0.1"
export OPENAI_TIMEOUT_SECONDS="30"
```

## üöÄ **API Endpoints**

### **Rate Limiting**
```bash
# Check user rate limits
GET /api/ai/monitoring/rate-limits/{userId}

# Reset user rate limits
POST /api/ai/monitoring/rate-limits/{userId}/reset
```

### **Token Usage**
```bash
# Get global token usage
GET /api/ai/monitoring/token-usage

# Get user-specific usage
GET /api/ai/monitoring/token-usage/{userId}

# Reset all counters
POST /api/ai/monitoring/token-usage/reset
```

### **Caching**
```bash
# Get cache statistics
GET /api/ai/monitoring/cache-stats
```

### **Cost Optimization**
```bash
# Get optimization tips
GET /api/ai/monitoring/cost-optimization
```

## üí∞ **Cost Savings Breakdown**

| Feature | Savings | Description |
|---------|---------|-------------|
| **GPT-5 Nano** | **40%** | Model ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏ß‡πà‡∏≤ gpt-4o-mini |
| **Caching** | **20-40%** | ‡∏•‡∏î API calls ‡∏ã‡πâ‡∏≥ |
| **Rate Limiting** | **Variable** | ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô |
| **Prompt Optimization** | **15-25%** | Prompt ‡∏™‡∏±‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û |
| **Token Monitoring** | **10-20%** | ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ |

### **Total Expected Savings: 60-80%** üéâ

## üìà **Usage Examples**

### **1. Basic AI Generation with Caching**
```java
@Autowired
private CachedAIService cachedAIService;

String content = cachedAIService.generateContentWithCache(
    "Create a flashcard about Java",
    "flashcard",
    "Keep it simple",
    "user123"
);
```

### **2. Check Rate Limits**
```java
@Autowired
private RateLimitService rateLimitService;

if (rateLimitService.canMakeRequest("user123")) {
    // Proceed with AI generation
} else {
    // Show rate limit message
}
```

### **3. Monitor Token Usage**
```java
@Autowired
private TokenUsageService tokenUsageService;

TokenUsageService.TokenUsageStats stats = tokenUsageService.getCurrentUsage();
TokenUsageService.CostEstimate cost = tokenUsageService.estimateCost();

System.out.println("Daily cost: $" + cost.getDailyCost());
System.out.println("Total tokens: " + stats.getTotalTokens());
```

## üîç **Monitoring Dashboard**

### **Real-time Metrics**
- Token usage per user
- Cost per day/month
- Cache hit rates
- Rate limit violations

### **Cost Alerts**
- High daily usage (>$0.10)
- Excessive requests (>20/day)
- Token usage spikes

### **Optimization Tips**
- Prompt improvement suggestions
- Caching recommendations
- Rate limit adjustments

## üß™ **Testing**

### **Run Cost Optimization Tests**
```bash
# Windows PowerShell
./test_cost_optimization.sh

# Or manually test endpoints
curl http://localhost:8080/api/ai/monitoring/token-usage
curl http://localhost:8080/api/ai/monitoring/rate-limits/test_user
```

### **Test Scenarios**
1. **Caching**: Same prompt ‚Üí Cache hit
2. **Rate Limiting**: Multiple rapid requests ‚Üí Limit enforcement
3. **Token Tracking**: API calls ‚Üí Token count increase
4. **Cost Calculation**: Usage ‚Üí Cost estimation

## ‚öôÔ∏è **Customization**

### **Adjust Rate Limits**
```yaml
ai:
  generation:
    rate-limit:
      max-requests-per-hour: 10    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 5 ‡πÄ‡∏õ‡πá‡∏ô 10
      max-requests-per-minute: 5   # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 3 ‡πÄ‡∏õ‡πá‡∏ô 5
      max-requests-per-day: 100    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 50 ‡πÄ‡∏õ‡πá‡∏ô 100
```

### **Cache Configuration**
```java
// ‡πÉ‡∏ô CacheConfig.java
.maximumSize(2000)           // ‡πÄ‡∏û‡∏¥‡πà‡∏° cache size
.expireAfterWrite(2, TimeUnit.HOURS)  // ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏¢‡∏∏
```

### **Token Monitoring Alerts**
```java
// ‡πÉ‡∏ô TokenUsageService.java
if (dailyCost > 0.20) {  // ‡πÄ‡∏û‡∏¥‡πà‡∏° threshold
    // Send alert
}
```

## üö® **Troubleshooting**

### **Common Issues**

1. **Cache Not Working**
   - Check Caffeine dependency in pom.xml
   - Verify @EnableCaching annotation
   - Check cache configuration

2. **Rate Limiting Too Strict**
   - Adjust limits in application.yml
   - Check user ID consistency
   - Verify timezone settings

3. **Token Usage Not Tracking**
   - Check OpenAI API response format
   - Verify TokenUsageService injection
   - Check logging for errors

### **Debug Commands**
```bash
# Check application logs
tail -f logs/application.log | grep "AI_Generate"

# Test individual services
curl -X POST http://localhost:8080/api/ai/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "test", "outputFormat": "flashcard"}'

# Monitor cache performance
curl http://localhost:8080/api/ai/monitoring/cache-stats
```

## üìö **Best Practices**

### **1. Caching Strategy**
- Cache similar prompts together
- Use appropriate TTL (Time To Live)
- Monitor cache hit rates
- Implement cache warming for popular requests

### **2. Rate Limiting**
- Set realistic limits based on user behavior
- Implement progressive limits (new users vs power users)
- Provide clear feedback when limits are hit
- Consider implementing burst allowances

### **3. Token Optimization**
- Keep prompts concise and specific
- Use few-shot examples when possible
- Implement prompt templates
- Monitor token usage patterns

### **4. Cost Monitoring**
- Set up daily cost alerts
- Track usage per user/feature
- Implement cost budgets
- Regular cost optimization reviews

## üîÆ **Future Enhancements**

### **Planned Features**
- **Advanced Caching**: Redis integration for distributed caching
- **Smart Rate Limiting**: ML-based limit adjustment
- **Cost Prediction**: AI-powered cost forecasting
- **Usage Analytics**: Detailed usage insights and trends
- **Automated Optimization**: Self-tuning parameters

### **Integration Possibilities**
- **Slack/Email Alerts**: Cost threshold notifications
- **Grafana Dashboards**: Real-time monitoring
- **Prometheus Metrics**: Performance tracking
- **AWS Cost Explorer**: Cloud cost integration

## üéØ **Success Metrics**

### **Key Performance Indicators**
- **Cost Reduction**: 60-80% savings target
- **Cache Hit Rate**: >70% for similar requests
- **API Response Time**: <2 seconds average
- **User Satisfaction**: <5% rate limit complaints
- **System Uptime**: >99.9% availability

### **Monitoring Checklist**
- [ ] Daily cost tracking
- [ ] Cache performance monitoring
- [ ] Rate limit violation alerts
- [ ] Token usage per user
- [ ] Cost optimization recommendations
- [ ] System performance metrics

## üèÅ **Getting Started**

### **1. Setup Dependencies**
```xml
<!-- Add to pom.xml -->
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-cache</artifactId>
</dependency>
<dependency>
    <groupId>com.github.ben-manes.caffeine</groupId>
    <artifactId>caffeine</artifactId>
</dependency>
```

### **2. Configure Environment**
```bash
export OPENAI_API_KEY="your-api-key"
export OPENAI_MODEL="gpt-5-nano"
```

### **3. Start Application**
```bash
mvn spring-boot:run
```

### **4. Test Features**
```bash
./test_cost_optimization.sh
```

### **5. Monitor Usage**
```bash
curl http://localhost:8080/api/ai/monitoring/token-usage
```

## üéâ **Conclusion**

‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì:

- **‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢ 60-80%** ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ OpenAI API
- **‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô** ‡∏î‡πâ‡∏ß‡∏¢ rate limiting ‡πÅ‡∏•‡∏∞ monitoring
- **‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û** ‡∏î‡πâ‡∏ß‡∏¢ caching ‡πÅ‡∏•‡∏∞ prompt optimization
- **‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢** ‡πÅ‡∏ö‡∏ö real-time
- **‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á UX** ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô

‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å setup! üöÄ
